{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishanthjois/2021_DeepLearning/blob/main/3_yoochoose2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7MU91xCXsJ5"
      },
      "outputs": [],
      "source": [
        "#download data\n",
        "!apt-get install p7zip\n",
        "!curl -Lo yoochoose-data.7z https://s3-eu-west-1.amazonaws.com/yc-rdata/yoochoose-data.7z\n",
        "!7z x yoochoose-data.7z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95toGBGpYLrT"
      },
      "outputs": [],
      "source": [
        "#installing packages\n",
        "!pip install git+https://github.com/maciejkula/spotlight.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVEPtfYyZnO8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import time\n",
        "import h5py\n",
        "import hashlib\n",
        "import json\n",
        "import shutil\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "from sklearn.model_selection import ParameterSampler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "random_state = np.random.RandomState(100)\n",
        "\n",
        "from spotlight.interactions import Interactions\n",
        "from spotlight.evaluation import mrr_score\n",
        "from spotlight.evaluation import precision_recall_score\n",
        "from spotlight.sequence.implicit import ImplicitSequenceModel\n",
        "from spotlight.sequence.representations import CNNNet\n",
        "from spotlight.evaluation import sequence_mrr_score\n",
        "from spotlight.cross_validation import random_train_test_split\n",
        "from spotlight.cross_validation import user_based_train_test_split\n",
        "from spotlight.factorization.implicit import ImplicitFactorizationModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaHoHSfRj7WS"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/sparsh9012/python-util.git\n",
        "sys.path.append('./python-util')\n",
        "sys.path.append('./python-util/recsys')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile preprocess.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# from reco.evaluate import user_item_crossjoin, filter_by\n",
        "\n",
        "\n",
        "def encode_user_item(df, user_col, item_col, rating_col, time_col):\n",
        "    \"\"\"Function to encode users and items\n",
        "    \n",
        "    Params:     \n",
        "        df (pd.DataFrame): Pandas data frame to be used.\n",
        "        user_col (string): Name of the user column.\n",
        "        item_col (string): Name of the item column.\n",
        "        rating_col (string): Name of the rating column.\n",
        "        timestamp_col (string): Name of the timestamp column.\n",
        "    \n",
        "    Returns: \n",
        "        encoded_df (pd.DataFrame): Modifed dataframe with the users and items index\n",
        "    \"\"\"\n",
        "    \n",
        "    encoded_df = df.copy()\n",
        "    \n",
        "    user_encoder = LabelEncoder()\n",
        "    user_encoder.fit(encoded_df[user_col].values)\n",
        "    n_users = len(user_encoder.classes_)\n",
        "    \n",
        "    item_encoder = LabelEncoder()\n",
        "    item_encoder.fit(encoded_df[item_col].values)\n",
        "    n_items = len(item_encoder.classes_)\n",
        "\n",
        "    encoded_df[\"USER\"] = user_encoder.transform(encoded_df[user_col])\n",
        "    encoded_df[\"ITEM\"] = item_encoder.transform(encoded_df[item_col])\n",
        "    \n",
        "    encoded_df.rename({rating_col: \"RATING\", time_col: \"TIMESTAMP\"}, axis=1, inplace=True)\n",
        "    \n",
        "    print(\"Number of users: \", n_users)\n",
        "    print(\"Number of items: \", n_items)\n",
        "    \n",
        "    return encoded_df, user_encoder, item_encoder\n",
        "\n",
        "\n",
        "def random_split (df, ratios, shuffle=False):\n",
        "    \n",
        "    \"\"\"Function to split pandas DataFrame into train, validation and test\n",
        "    \n",
        "    Params:     \n",
        "        df (pd.DataFrame): Pandas data frame to be split.\n",
        "        ratios (list of floats): list of ratios for split. The ratios have to sum to 1.\n",
        "    \n",
        "    Returns: \n",
        "        list: List of pd.DataFrame split by the given specifications.\n",
        "    \"\"\"\n",
        "    seed = 42                  # Set random seed\n",
        "    if shuffle == True:\n",
        "        df = df.sample(frac=1)     # Shuffle the data\n",
        "    samples = df.shape[0]      # Number of samples\n",
        "    \n",
        "    # Converts [0.7, 0.2, 0.1] to [0.7, 0.9]\n",
        "    split_ratio = np.cumsum(ratios).tolist()[:-1] # Get split index\n",
        "    \n",
        "    # Get the rounded integer split index\n",
        "    split_index = [round(x * samples) for x in split_ratio]\n",
        "    \n",
        "    # split the data\n",
        "    splits = np.split(df, split_index)\n",
        "    \n",
        "    # Add split index (this makes splitting by group more efficient).\n",
        "    for i in range(len(ratios)):\n",
        "        splits[i][\"split_index\"] = i\n",
        "\n",
        "    return splits\n",
        "\n",
        "\n",
        "def user_split (df, ratios, chrono=False):\n",
        "    \n",
        "    \"\"\"Function to split pandas DataFrame into train, validation and test (by user in chronological order)\n",
        "    \n",
        "    Params:     \n",
        "        df (pd.DataFrame): Pandas data frame to be split.\n",
        "        ratios (list of floats): list of ratios for split. The ratios have to sum to 1.\n",
        "        chrono (boolean): whether to sort in chronological order or not\n",
        "    \n",
        "    Returns: \n",
        "        list: List of pd.DataFrame split by the given specifications.\n",
        "    \"\"\"\n",
        "    seed = 42                  # Set random seed\n",
        "    samples = df.shape[0]      # Number of samples\n",
        "    col_time = \"TIMESTAMP\"\n",
        "    col_user = \"USER\"\n",
        "    \n",
        "    # Split by each group and aggregate splits together.\n",
        "    splits = []\n",
        "\n",
        "    # Sort in chronological order, the split by users\n",
        "    if chrono == True:\n",
        "        df_grouped = df.sort_values(col_time).groupby(col_user)\n",
        "    else:\n",
        "        df_grouped = df.groupby(col_user)\n",
        "\n",
        "        \n",
        "    \n",
        "    for name, group in df_grouped:\n",
        "        group_splits = random_split(df_grouped.get_group(name), ratios, shuffle=False)\n",
        "        \n",
        "        # Concatenate the list of split dataframes.\n",
        "        concat_group_splits = pd.concat(group_splits)\n",
        "        splits.append(concat_group_splits)\n",
        "    \n",
        "    # Concatenate splits for all the groups together.\n",
        "    splits_all = pd.concat(splits)\n",
        "\n",
        "    # Take split by split_index\n",
        "    splits_list = [ splits_all[splits_all[\"split_index\"] == x] for x in range(len(ratios))]\n",
        "\n",
        "    return splits_list\n",
        "\n",
        "def neg_feedback_samples(\n",
        "    df,\n",
        "    rating_threshold, \n",
        "    ratio_neg_per_user=1\n",
        "):\n",
        "    \"\"\" function to sample negative feedback from user-item interaction dataset.\n",
        "\n",
        "    This negative sampling function will take the user-item interaction data to create \n",
        "    binarized feedback, i.e., 1 and 0 indicate positive and negative feedback, \n",
        "    respectively. \n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): input data that contains user-item tuples.\n",
        "        rating_threshold (int): value below which feedback is set to 0 and above which feedback is set to 1\n",
        "        ratio_neg_per_user (int): ratio of negative feedback w.r.t to the number of positive feedback for each user. \n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: data with negative feedback \n",
        "    \"\"\"\n",
        "    \n",
        "    #df.rename({\"user_id\":\"USER\", \"movie_id\":\"ITEM\", \"rating\":\"RATING\"}, inplace=True)\n",
        "    #print(df.columns)\n",
        "    #print(df.columns)\n",
        "    df.columns = [\"USER\", \"ITEM\", \"RATING\", \"unix_timestamp\"]\n",
        "    #print(df.columns)\n",
        "    \n",
        "    seed = 42\n",
        "    \n",
        "    df_pos = df.copy()\n",
        "    df_pos[\"RATING\"] = df_pos[\"RATING\"].apply(lambda x: 1 if x >= rating_threshold else 0)\n",
        "    df_pos = df_pos[df_pos.RATING>0]\n",
        "\n",
        "\n",
        "    # Create a dataframe for all user-item pairs \n",
        "    df_neg = user_item_crossjoin(df)\n",
        "\n",
        "    #remove positive samples from the cross-join dataframe\n",
        "    df_neg = filter_by(df_neg, df_pos, [\"USER\", \"ITEM\"])    \n",
        "\n",
        "    #Add a column for rating - setting it to 0\n",
        "    df_neg[\"RATING\"] = 0\n",
        "   \n",
        "    # Combine positive and negative samples into a single dataframe\n",
        "    df_all = pd.concat([df_pos, df_neg], ignore_index=True, sort=True)\n",
        "    df_all = df_all[[\"USER\", \"ITEM\", \"RATING\"]]\n",
        "    \n",
        "    \n",
        "    # Sample negative feedback from the combined dataframe.\n",
        "    df_sample = (\n",
        "        df_all.groupby(\"USER\")\n",
        "        .apply(\n",
        "            lambda x: pd.concat(\n",
        "                [\n",
        "                    x[x[\"RATING\"] == 1],\n",
        "                    x[x[\"RATING\"] == 0].sample(\n",
        "                        min(\n",
        "                            max(\n",
        "                                round(len(x[x[\"RATING\"] == 1]) * ratio_neg_per_user), 1\n",
        "                            ),\n",
        "                            len(x[x[\"RATING\"] == 0]),\n",
        "                        ),\n",
        "                        random_state=seed,\n",
        "                        replace=False,\n",
        "                    )\n",
        "                    if len(x[x[\"RATING\"] == 0] > 0)\n",
        "                    else pd.DataFrame({}, columns=[\"USER\", \"ITEM\", \"RATING\"]),\n",
        "                ],\n",
        "                ignore_index=True,\n",
        "                sort=True,\n",
        "            )\n",
        "        )\n",
        "        .reset_index(drop=True)\n",
        "        .sort_values(\"USER\")\n",
        "    )\n",
        "\n",
        "#     print(\"####\")\n",
        "#     print(df_sample.columns)\n",
        "#     print(df.columns)\n",
        "#     df_sample_w_ts = pd.merge(df_sample, df, on=[\"USER\", \"ITEM\"], how=\"left\")\n",
        "#     print(df_sample.columns)\n",
        "    df_sample.columns = [\"movie_id\", \"rating\", \"user_id\"]\n",
        "    return df_sample[[\"user_id\", \"movie_id\", \"rating\"]]\n",
        "#    return df_sample\n",
        "\n",
        "\n",
        "def sample_data():\n",
        "\n",
        "    data = pd.DataFrame({\n",
        "        \"user_index\": [1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3],\n",
        "        \"item_index\": [1, 1, 2, 2, 2, 1, 2, 1, 2, 3, 3, 3, 3, 3, 1],\n",
        "        \"rating\": [4, 4, 3, 3, 3, 4, 5, 4, 5, 5, 5, 5, 5, 5, 4],\n",
        "        \"timestamp\": [\n",
        "            '2000-01-01', '2000-01-01', '2000-01-02', '2000-01-02', '2000-01-02',\n",
        "            '2000-01-01', '2000-01-01', '2000-01-03', '2000-01-03', '2000-01-03',\n",
        "            '2000-01-01', '2000-01-03', '2000-01-03', '2000-01-03', '2000-01-04'\n",
        "        ]\n",
        "    })\n",
        "    \n",
        "    return data"
      ],
      "metadata": {
        "id": "kVeKGPKsYSsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import SVG, display\n",
        "from preprocess import encode_user_item, random_split, user_split"
      ],
      "metadata": {
        "id": "bMieYOuvYu9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_clicks1 = pd.read_csv('yoochoose-clicks.dat')\n",
        "df_clicks1.head()"
      ],
      "metadata": {
        "id": "kMfAKDpRnztB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCTMKQG9cH5F"
      },
      "source": [
        "Clicks data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLDGuRyVZuV9"
      },
      "outputs": [],
      "source": [
        "df_clicks = pd.read_csv('yoochoose-clicks.dat', sep=',', header=None,\n",
        "                        dtype={0:np.int32, 1:str, 2:np.int64, 3:str},\n",
        "                        names = [\"SessionId\", \"TimeStr\", \"ItemId\", \"Item_Type\"])\n",
        "df_clicks.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRgFEPZIaF5j"
      },
      "outputs": [],
      "source": [
        "#category types\n",
        "'''The categories can be S (for promotion), 0 (when unknown), \n",
        "a number between 1-12 when it came from a category on the page\n",
        "or any other that represents a brand'''\n",
        "\n",
        "def assign_cat(x):\n",
        "    if x == \"S\":\n",
        "        return \"PROMOTION\"\n",
        "    elif np.int(x) == 0:\n",
        "        return \"NONE\"\n",
        "    elif np.int(x) < 13:\n",
        "        return \"CATEGORY\"\n",
        "    else:\n",
        "        return \"BRAND\"\n",
        "\n",
        "df_clicks['Item_Type'] = df_clicks.loc[:,'Item_Type'].map(assign_cat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8To1GTU-cKiQ"
      },
      "source": [
        "Buy data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C4NXwi1boWe"
      },
      "outputs": [],
      "source": [
        "df_buys = pd.read_csv('yoochoose-buys.dat', sep=',', header=None,\n",
        "                      dtype={0:np.int32, 1:str, 2:np.int64, \n",
        "                             3:np.int64, 4:np.int64},\n",
        "                      names = [\"SessionId\", \"TimeStr\", \"ItemId\", \"Price\", \"Quantity\"])\n",
        "df_buys.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v0C5RQ7cTN4"
      },
      "outputs": [],
      "source": [
        "df_buys.drop([\"TimeStr\"], inplace=True, axis=1)\n",
        "df_buys[\"Action\"] = \"BUY\"\n",
        "df_buys.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHeJpa2Qcz2c"
      },
      "outputs": [],
      "source": [
        "df = pd.merge(left=df_clicks, right=df_buys, how=\"left\", on=[\"SessionId\", \"ItemId\"])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEB_LsKsda3L"
      },
      "source": [
        "Exploring data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BA5hyyqsc9wB"
      },
      "outputs": [],
      "source": [
        "query = \"ItemId==@ItemId & SessionId==@SessionId\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-31oAd1cddJn"
      },
      "outputs": [],
      "source": [
        "ItemId = 214821371\n",
        "SessionId = 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0G73pO-deOq"
      },
      "outputs": [],
      "source": [
        "df_clicks.query(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0Sw-hNPdjXg"
      },
      "outputs": [],
      "source": [
        "df_buys.query(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzMW0Hv5doV0"
      },
      "outputs": [],
      "source": [
        "df.query(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qlE39O-d0Zh"
      },
      "outputs": [],
      "source": [
        "# Drop duplicates\n",
        "df.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZAqz9yvd9_z"
      },
      "source": [
        "Data subset selection based on thresholds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "878YBWnxd2z8"
      },
      "outputs": [],
      "source": [
        "SESSION_THRESHOLD = 20\n",
        "ITEM_THRESHOLD = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EV3h9se0eC23"
      },
      "outputs": [],
      "source": [
        "session_lengths = df.groupby([\"SessionId\"]).size()\n",
        "session_lengths_w_threshold = (session_lengths[session_lengths>SESSION_THRESHOLD]).reset_index()\n",
        "df_with_session_threshold = df[df.SessionId.isin(session_lengths_w_threshold.SessionId)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVfFuq6AeUvC"
      },
      "outputs": [],
      "source": [
        "item_lengths = df.groupby([\"ItemId\"]).size()\n",
        "item_lengths_w_threshold = item_lengths[item_lengths>ITEM_THRESHOLD].reset_index()\n",
        "df_with_session_item_threshold = df_with_session_threshold[df_with_session_threshold.ItemId.isin(item_lengths_w_threshold.ItemId)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IltbgpfFeed9"
      },
      "outputs": [],
      "source": [
        "session_lengths_2 = df_with_session_item_threshold.groupby([\"SessionId\"]).size()\n",
        "session_lengths_2_w_threshold = (session_lengths_2[session_lengths_2 > SESSION_THRESHOLD]).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmc09qDfe5YV"
      },
      "outputs": [],
      "source": [
        "df_final = df_with_session_item_threshold[df_with_session_item_threshold.SessionId.isin(session_lengths_2_w_threshold.SessionId)]\n",
        "df_final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG5aglk3e7Qx"
      },
      "outputs": [],
      "source": [
        "df_final.Action.fillna(value=\"CLICK\", inplace=True)\n",
        "df_final.drop([\"Price\", \"Quantity\"], axis=1, inplace=True)\n",
        "df_final.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbrd6VAtfXvp"
      },
      "source": [
        "Some more changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiHlzI-sfNKx"
      },
      "outputs": [],
      "source": [
        "df_final['Time'] = df_final.TimeStr.apply(lambda x: dt.datetime.strptime(x, '%Y-%m-%dT%H:%M:%S.%fZ').timestamp())\n",
        "del(df_final[\"TimeStr\"])\n",
        "df_final.sort_values(by=[\"SessionId\", \"Time\"], inplace=True)\n",
        "df_final[\"Rating\"] = df_final.Action.apply(lambda x: 1 if (x == \"CLICK\") else 5)\n",
        "df_final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHQWlBsgmYnj"
      },
      "outputs": [],
      "source": [
        "df_final.to_csv('yoochoose_processed.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmOAig-Tmw2n"
      },
      "source": [
        "Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1NuQgmkmv7y"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"yoochoose_processed.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0AP3kMdgAe7"
      },
      "outputs": [],
      "source": [
        "# Data Encoding\n",
        "DATA, user_encoder, item_encoder = encode_user_item(df, \"SessionId\", \"ItemId\", \"Rating\", \"Time\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKMhEWG-mNOp"
      },
      "outputs": [],
      "source": [
        "# Spotlight requires encoders to begin from 1 (instead of 0). We will add 1 to the encoders \n",
        "# When doing inverse transform, remember to subtract 1.\n",
        "\n",
        "DATA.USER = DATA.USER + 1\n",
        "DATA.ITEM = DATA.ITEM + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5NwDqbvnHBK"
      },
      "outputs": [],
      "source": [
        "DATA.RATING = DATA.RATING.astype(np.int32)\n",
        "DATA.USER = DATA.USER.astype(np.int32)\n",
        "DATA.ITEM = DATA.ITEM.astype(np.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4vU005DnHAO"
      },
      "outputs": [],
      "source": [
        "DATA.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-MRE1RFnG-y"
      },
      "outputs": [],
      "source": [
        "df_for_interaction_matrix = (DATA.USER.values,DATA.ITEM.values,DATA.RATING,DATA.TIMESTAMP)\n",
        "df_interaction = Interactions(*df_for_interaction_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTWAF0ejnOde"
      },
      "source": [
        "Train and Validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bori0cEanQK9"
      },
      "outputs": [],
      "source": [
        "train_with_val, test = user_based_train_test_split(df_interaction,\n",
        "                                                   random_state=random_state, \n",
        "                                                   test_percentage = 0.2)\n",
        "\n",
        "train, val = user_based_train_test_split(train_with_val, test_percentage=0.2, \n",
        "                                         random_state=random_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuBsjgBcnupT"
      },
      "source": [
        "Implicit Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HDyMDY6nsfW"
      },
      "outputs": [],
      "source": [
        "model_implicit = ImplicitFactorizationModel(n_iter=3, loss='bpr')\n",
        "model_implicit.fit(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "do38N6cvnz62"
      },
      "outputs": [],
      "source": [
        "user_for_reco = test.user_ids[0]\n",
        "pred_for_user = model_implicit.predict(user_for_reco)\n",
        "pred_for_user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySbJSOKJn87d"
      },
      "outputs": [],
      "source": [
        "rec_item_ids = (-pred_for_user).argsort()\n",
        "rec_item_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dm79pGTFoApO"
      },
      "outputs": [],
      "source": [
        "# ground truth\n",
        "target = test.item_ids[0]\n",
        "target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULJJqSENoB4Z"
      },
      "outputs": [],
      "source": [
        "np.where(rec_item_ids == target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUusie0zoH3L"
      },
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6Ufi-enoCOI"
      },
      "outputs": [],
      "source": [
        "implicit_mrr_score = mrr_score(model_implicit, test)\n",
        "(pk, rk) = precision_recall_score(model_implicit, test, k= 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmDyf4tYoRTm"
      },
      "source": [
        "Sequence Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnbvJzlCoQXX"
      },
      "outputs": [],
      "source": [
        "max_sequence_length = 200\n",
        "min_sequence_length = 50\n",
        "step_size = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kq6LESTloZEs"
      },
      "outputs": [],
      "source": [
        "train = train.to_sequence(max_sequence_length=max_sequence_length,\n",
        "                          min_sequence_length=min_sequence_length,\n",
        "                          step_size=step_size)\n",
        "test = test.to_sequence(max_sequence_length=max_sequence_length,\n",
        "                        min_sequence_length=min_sequence_length,\n",
        "                        step_size=step_size)\n",
        "val = val.to_sequence(max_sequence_length=max_sequence_length,\n",
        "                                    min_sequence_length=min_sequence_length,\n",
        "                                    step_size=step_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD1eWb7toaCK"
      },
      "outputs": [],
      "source": [
        "print(train.sequences.shape)\n",
        "print(test.sequences.shape)\n",
        "print(val.sequences.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6a02DcGobP3"
      },
      "outputs": [],
      "source": [
        "net = CNNNet(train.num_items,\n",
        "             embedding_dim=128,\n",
        "             kernel_width=3,\n",
        "             dilation=[1,1,1,1],\n",
        "             num_layers=2,\n",
        "             nonlinearity=\"relu\",\n",
        "             residual_connections=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHwPicjbocLo"
      },
      "outputs": [],
      "source": [
        "model = ImplicitSequenceModel(loss=\"bpr\",\n",
        "                              representation=net,\n",
        "                              batch_size=32,\n",
        "                              learning_rate=0.1,\n",
        "                              l2=0.0,\n",
        "                              n_iter=2,\n",
        "                              random_state=random_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sm2VYVNGodRW"
      },
      "outputs": [],
      "source": [
        "model.fit(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chWvWfVWofOu"
      },
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-I2Vtzp9oesZ"
      },
      "outputs": [],
      "source": [
        "query = test.sequences[1][0:199]\n",
        "target = test.sequences[1][199]\n",
        "\n",
        "print(\"Shape of query is : \",query.shape)\n",
        "print(\"The value of target is : \",target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtZIMdJPohcd"
      },
      "outputs": [],
      "source": [
        "pred = model.predict(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJY9Id1kojDC"
      },
      "outputs": [],
      "source": [
        "rec_item_ids = (-pred).argsort()\n",
        "np.where(rec_item_ids == target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6WnSV2feomV3"
      },
      "outputs": [],
      "source": [
        "#Item ID that is to be recommended :\n",
        "item_encoder.inverse_transform([rec_item_ids[5]-1])[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Item ID that is to be recommended :\n",
        "item_encoder.inverse_transform([rec_item_ids[1]-1])[0]"
      ],
      "metadata": {
        "id": "5pveDwKglmVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rec_item_ids"
      ],
      "metadata": {
        "id": "Uv5ApBRQlKAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_final.loc[df_final['SessionId'] == 87]\n",
        "\n"
      ],
      "metadata": {
        "id": "_rsh-BRXmejf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "3-yoochoose2.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}